{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import numpy\n",
    "import tensorflow\n",
    "from tensorflow.python.framework import ops\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from numpy import unique\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  81\n",
      "['doit', 'fine', 'goodbye', 'greeting', 'joke', 'mood', 'name', 'ok', 'thanks', 'weather']\n",
      "Number of tags:  10\n"
     ]
    }
   ],
   "source": [
    "with open(\"/intents.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "defaults = []\n",
    "\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        # делит на слова и знаки\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend(wrds)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent[\"tag\"])\n",
    "\n",
    "    if intent[\"tag\"] not in labels:\n",
    "        labels.append(intent[\"tag\"])\n",
    "        \n",
    "for default in data[\"defaults\"]:\n",
    "    defaults.append(default)\n",
    "    \n",
    "#print(words)\n",
    "\n",
    "# массив всех известных слов - words\n",
    "\n",
    "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "print(\"Number of words: \",len(words))\n",
    "\n",
    "labels = sorted(labels)\n",
    "\n",
    "print(labels)\n",
    "print(\"Number of tags: \",len(labels))\n",
    "\n",
    "# training - это  список массивов с 0 и 1, где 1 - это вхождение слова в большой список всех слов \n",
    "training = []\n",
    "# для кождого массива - массив из 0 и 1, где 1 - напротив нужного класса (тэга)\n",
    "output = []\n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "\n",
    "    wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "\n",
    "\n",
    "training = numpy.array(training)\n",
    "output = numpy.array(output)\n",
    "   \n",
    "\n",
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    return numpy.array(bag)\n",
    "\n",
    "def chat():\n",
    "    print(\"Start talking with the bot (type quit to stop)!\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        bag = bag_of_words(inp, words)\n",
    "        if len(unique(bag)) <= 1:\n",
    "            print(random.choice(defaults)) \n",
    "        else:\n",
    "            results = model.predict([bag])\n",
    "            results_index = numpy.argmax(results)\n",
    "            tag = labels[results_index]\n",
    "\n",
    "            for tg in data[\"intents\"]:\n",
    "                if tg['tag'] == tag:\n",
    "                    responses = tg['responses']\n",
    "\n",
    "            print(random.choice(responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(training, output, test_size=0.1, random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:   51.3s\n",
      "[Parallel(n_jobs=-1)]: Done 605 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 85, 'max_features': 'auto', 'max_depth': 73}\n"
     ]
    }
   ],
   "source": [
    "# для подбора параметров\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "# есть слово или нет в предложении - мои фичи \n",
    "# каждое дерево берет рандомное количество фич\n",
    "\n",
    "# number of trees in random forest\n",
    "n_estimators = [int(x) for x in numpy.linspace(start = 20, stop = 300, num = 100)]\n",
    "# number of features at every split\n",
    "max_features = ['auto', 'log2']\n",
    "\n",
    "# max depth\n",
    "max_depth = [int(x) for x in numpy.linspace(1, 100, num = 100)]\n",
    "max_depth.append(None)\n",
    "# create random grid\n",
    "random_grid = {\n",
    " 'n_estimators': n_estimators,\n",
    " 'max_features': max_features,\n",
    " 'max_depth': max_depth\n",
    " }\n",
    "# Random search of parameters\n",
    "rfc_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 100, cv = 10, verbose=4, random_state=42, n_jobs = -1)\n",
    "# Fit the model\n",
    "rfc_random.fit(X_train, y_train)\n",
    "# print results\n",
    "print(rfc_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       1.00      1.00      1.00         1\n",
      "           5       1.00      1.00      1.00         1\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       1.00      0.62      0.77         8\n",
      "   macro avg       0.40      0.40      0.40         8\n",
      "weighted avg       0.62      0.62      0.62         8\n",
      " samples avg       0.62      0.62      0.62         8\n",
      "\n",
      "Random Forest train score:  0.9552238805970149\n",
      "Random Forest test score: 0.625\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=76, max_depth=33, max_features='auto')\n",
    "rfc.fit(X_train,y_train)\n",
    "rfc_predict = rfc.predict(X_test)\n",
    "rfc_cv_score = cross_val_score(rfc, training, output, cv=10)\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, rfc_predict))\n",
    "print(\"Random Forest train score: \", rfc.score(X_train, y_train))\n",
    "print(\"Random Forest test score:\", rfc.score(X_test, y_test))\n",
    "model = rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start talking with the bot (type quit to stop)!\n",
      "You: weather\n",
      "Weather? I can't forecast it for tomorrow, but I like it today\n",
      "You: hi\n",
      "Hello there\n",
      "You: how are you\n",
      "I am good, thanks\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}